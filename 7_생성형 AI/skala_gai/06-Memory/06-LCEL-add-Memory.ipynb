{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY Loading\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "CH06-Memory\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"CH06-Memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory with Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/2rwxb6k520l4tqxzpx1nh8lh0000gn/T/ipykernel_31628/4009527432.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ ì •ì˜/ì´ˆê¸°í™” \n",
    "model = ChatOpenAI()\n",
    "\n",
    "# ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ì •ì˜ \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','Your are a helpful assistant.'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ì˜/ì´ˆê¸°í™”\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì´ˆê¸°í™”ëœ ë©”ëª¨ë¦¬ í™•ì¸ \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunnableLambda : ê´„í˜¸ ë‚´ í•¨ìˆ˜ í˜¸ì¶œ \n",
    "# memory.load_memory_variables : ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥, ê²°ê³¼ëŠ” chat_history ë³€ìˆ˜ì— ì €ì¥ \n",
    "# itemgetter : chat_history ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°€ì ¸ì˜¤ë„ë¡ ì²˜ë¦¬ \n",
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) \n",
    "    | itemgetter('chat_history')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello', 'chat_history': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# runnable ê²°ê³¼ í™•ì¸ \n",
    "runnable.invoke({'input':'Hello'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²´ì¸ ì •ì˜\n",
    "chain = runnable | prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì˜í¬ë‹˜. ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì²«ë²ˆì§¸ ëŒ€í™” \n",
    "answer = chain.invoke({'input':'ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ì˜í¬ ì…ë‹ˆë‹¤. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.'})\n",
    "answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ì˜í¬ ì…ë‹ˆë‹¤. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì˜í¬ë‹˜. ë§Œë‚˜ì„œ ë°˜ê°€ì›Œìš”. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ëŒ€í™” ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ì €ì¥ \n",
    "memory.save_context(\n",
    "    {'human':'ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ì˜í¬ ì…ë‹ˆë‹¤. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.'},\n",
    "    {'ai':answer.content},\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë„¤, ê¹€ì˜í¬ë‹˜ì´ì‹œì£ . ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë‘ë²ˆì§¸ ëŒ€í™” \n",
    "answer = chain.invoke({'input':'ì œ ì´ë¦„ ê¸°ì–µí•˜ì„¸ìš”?'})\n",
    "answer.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Conversation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationChainClass(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory, input_key=\"input\"):\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables)\n",
    "                | itemgetter(memory.memory_key)  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({self.input_key: query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™” \n",
    "model = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model_name='gpt-4o-mini',\n",
    ")\n",
    "\n",
    "# ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ì •ì˜ \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','Your are a helpful assistant.'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ì˜/ì´ˆê¸°í™” \n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key='chat_history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_chain = ConversationChainClass(model, prompt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì˜í¬ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke('ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ì˜í¬ ì…ë‹ˆë‹¤. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë„¤, ê¹€ì˜í¬ë‹˜! ë‹¹ì‹ ì˜ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹  ê²ƒì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke('ì œ ì´ë¦„ ê¸°ì–µí•˜ì„¸ìš”?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í˜ë‚´ì„¸ìš”, ê¹€ì˜í¬ë‹˜! LCEL ë¬¸ë²• ìˆ˜ì—…ì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ì—¬ëŸ¬ë¶„ì˜ ì‹¤ë ¥ì´ ì‘¥ì‘¥ ì„±ì¥í•˜ëŠ” ê³¼ì •ì´ì—ìš”. ì¡°ê¸ˆì”© ë‚˜ì•„ì§€ëŠ” ìì‹ ì„ ë¯¿ê³ , í¬ê¸°í•˜ì§€ ë§ˆì„¸ìš”! ëª¨ë“  ë…¸ë ¥ì€ ê²°êµ­ ì¢‹ì€ ê²°ê³¼ë¡œ ëŒì•„ì˜¬ ê±°ì˜ˆìš”. í™”ì´íŒ…! ğŸ˜Š'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke('ì˜¤ëŠ˜ LCEL ë¬¸ë²• ìˆ˜ì—… ë“£ê³  ìˆëŠ”ë° ë„ˆë¬´ í˜ë“¤ì–´ìš”. í˜ë‚˜ëŠ” ë©˜íŠ¸ ì£¼ì„¸ìš”!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ê¹€ì˜í¬ ì…ë‹ˆë‹¤. ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ê¹€ì˜í¬ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì œ ì´ë¦„ ê¸°ì–µí•˜ì„¸ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ë„¤, ê¹€ì˜í¬ë‹˜! ë‹¹ì‹ ì˜ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ì´ í•„ìš”í•˜ì‹  ê²ƒì´ ìˆìœ¼ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì˜¤ëŠ˜ LCEL ë¬¸ë²• ìˆ˜ì—… ë“£ê³  ìˆëŠ”ë° ë„ˆë¬´ í˜ë“¤ì–´ìš”. í˜ë‚˜ëŠ” ë©˜íŠ¸ ì£¼ì„¸ìš”!!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='í˜ë‚´ì„¸ìš”, ê¹€ì˜í¬ë‹˜! LCEL ë¬¸ë²• ìˆ˜ì—…ì€ ì–´ë ¤ìš¸ ìˆ˜ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ì—¬ëŸ¬ë¶„ì˜ ì‹¤ë ¥ì´ ì‘¥ì‘¥ ì„±ì¥í•˜ëŠ” ê³¼ì •ì´ì—ìš”. ì¡°ê¸ˆì”© ë‚˜ì•„ì§€ëŠ” ìì‹ ì„ ë¯¿ê³ , í¬ê¸°í•˜ì§€ ë§ˆì„¸ìš”! ëª¨ë“  ë…¸ë ¥ì€ ê²°êµ­ ì¢‹ì€ ê²°ê³¼ë¡œ ëŒì•„ì˜¬ ê±°ì˜ˆìš”. í™”ì´íŒ…! ğŸ˜Š', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë©”ëª¨ë¦¬ í™•ì¸ \n",
    "conversation_chain.memory.load_memory_variables({})[\"chat_history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "** End of Documents **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-t0JhnSEV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
