{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93743ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경변수 가져오기\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# 또는 다음과 같이 직접 키 입력 (개발)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # 자신의 OpenAI 키\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeb0be",
   "metadata": {},
   "source": [
    "#### 다음 실습 코드는 학습 목적으로만 사용 바랍니다. 문의 : audit@korea.ac.kr 임성열 Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5d4b1",
   "metadata": {},
   "source": [
    "#### LangGraph 기반 LangChain Agent 구현\n",
    "이 노트북은 다음의 Tool들을 사용하는 LangGraph Agent를 구현하고 실행합니다:\n",
    "\n",
    "1. **사칙연산 함수**\n",
    "2. **PDF 업로드 및 임베딩 후 FAISS 저장**\n",
    "3. **웹 크롤링을 통한 트렌드 저장**\n",
    "4. **벡터 DB로부터 요약 및 등장 횟수 카운트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치 (필요 시 주석 제거)\n",
    "%pip install langchain langgraph faiss-cpu openai PyMuPDF beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3746e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bbbd",
   "metadata": {},
   "source": [
    "#### 1. 사칙연산 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e7975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"사칙연산을 수행한다. 예: '2 + 3 * 4'\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"{expression} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"오류: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542d2fe",
   "metadata": {},
   "source": [
    "#### 2. PDF 임베딩 및 벡터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15973137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 전역 벡터 DB\n",
    "pdf_vectorstore = None\n",
    "\n",
    "@tool\n",
    "def embed_pdf_to_faiss(path: str) -> str:\n",
    "    \"\"\"\n",
    "    PDF 파일의 상대 경로 또는 절대 경로를 받아 절대 경로로 변환한 후,\n",
    "    해당 PDF를 임베딩하여 벡터 DB에 저장합니다.\n",
    "    \"\"\"\n",
    "    global pdf_vectorstore\n",
    "\n",
    "    # 절대 경로로 변환\n",
    "    abs_path = os.path.abspath(path)\n",
    "\n",
    "    if not os.path.exists(abs_path):\n",
    "        return f\"❌ PDF 파일이 존재하지 않습니다: {abs_path}\"\n",
    "\n",
    "    loader = PyMuPDFLoader(abs_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pdf_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    return f\"✅ '{abs_path}' PDF가 벡터 DB에 저장되었습니다.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd45d88",
   "metadata": {},
   "source": [
    "#### 3. 네이버 트렌드 크롤링 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99d74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_trend_and_store(keyword: str) -> str:\n",
    "#     \"\"\"네이버 검색 트렌드를 크롤링하고 벡터 DB에 저장한다.\"\"\"\n",
    "#     global web_vectorstore\n",
    "#     url = f\"https://search.naver.com/search.naver?query={keyword}\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "#     contents = [keyword + \":\\n\" + p for p in paragraphs if keyword.lower() in p.lower()]\n",
    "#     documents = [Document(page_content=c) for c in contents]\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     web_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "#     return f\"✅ '{keyword}' 검색결과가 벡터 DB에 저장되었습니다.\"\n",
    "\n",
    "# 3️⃣ 네이버 검색 크롤링 + 임베딩\n",
    "@tool\n",
    "def crawl_naver_and_embed(term: str) -> str:\n",
    "    \"\"\"\n",
    "    네이버 뉴스 API를 사용해 특정 키워드로 뉴스 기사를 검색하고,\n",
    "    그 결과를 임베딩하여 FAISS 벡터 DB에 저장합니다.\n",
    "    \"\"\"\n",
    "    global web_vectorstore\n",
    "\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": \"gbqzUVViEiF6WXhuq3gZ\",\n",
    "        \"X-Naver-Client-Secret\": \"y0YXaa5unU\"\n",
    "    }\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
    "    params = {\n",
    "        \"query\": term,\n",
    "        \"display\": 5,\n",
    "        \"sort\": \"date\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        return f\"❌ 네이버 API 호출 실패: {e}\"\n",
    "\n",
    "    data = response.json()\n",
    "    items = data.get(\"items\", [])\n",
    "\n",
    "    if not items:\n",
    "        return \"🔍 검색 결과가 없습니다.\"\n",
    "\n",
    "    documents = []\n",
    "    for item in items:\n",
    "        title = re.sub(r\"<.*?>\", \"\", item.get(\"title\", \"\"))\n",
    "        description = re.sub(r\"<.*?>\", \"\", item.get(\"description\", \"\"))\n",
    "        content = f\"{title}\\n{description}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": \"NaverNews\"}))\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    web_vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    return f\"✅ '{term}'에 대한 뉴스 트렌드가 벡터 DB에 저장되었습니다.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3d9d0",
   "metadata": {},
   "source": [
    "#### 4. PDF와 웹 벡터 DB 요약 및 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb331cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def summarize_and_count(keyword: str) -> str:\n",
    "    \"\"\"PDF와 웹에서 해당 키워드를 요약하고 등장 횟수를 계산\"\"\"\n",
    "    pdf_docs = pdf_vectorstore.similarity_search(keyword, k=10)\n",
    "    web_docs = web_vectorstore.similarity_search(keyword, k=10)\n",
    "\n",
    "    pdf_text = \"\\n\".join([doc.page_content for doc in pdf_docs])\n",
    "    web_text = \"\\n\".join([doc.page_content for doc in web_docs])\n",
    "\n",
    "    def count_keyword(text):\n",
    "        return len(re.findall(keyword, text, re.IGNORECASE))\n",
    "\n",
    "    count_pdf = count_keyword(pdf_text)\n",
    "    count_web = count_keyword(web_text)\n",
    "\n",
    "    return f\"📘 PDF 요약: {pdf_text[:300]}...\\n🕸 웹 트렌드 요약: {web_text[:300]}...\\n'📊 등장 횟수 - PDF: {count_pdf}회, 웹: {count_web}회\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2fbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /Users/phoenix/Eagle/2025_LangChain/langchain_lab\n",
      "attention.pdf 존재 여부: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"현재 작업 디렉토리:\", os.getcwd())\n",
    "print(\"attention.pdf 존재 여부:\", os.path.exists(\"attention.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda72fc",
   "metadata": {},
   "source": [
    "#### 5. LangGraph Agent 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/0xbgl9wd3wz914nd58ylmykw0000gp/T/ipykernel_78154/2527526929.py:30: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  executor = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `embed_pdf_to_faiss` with `{'path': './attention.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m✅ '/Users/phoenix/Eagle/2025_LangChain/langchain_lab/attention.pdf' PDF가 벡터 DB에 저장되었습니다.\u001b[0m\u001b[32;1m\u001b[1;3m'./attention.pdf' 파일이 벡터 DB에 성공적으로 저장되었습니다. 다른 도움이 필요하시면 알려주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"다음 PDF 파일을 벡터 DB에 저장하라. 파일 경로는 './attention.pdf' 이다.\", 'output': \"'./attention.pdf' 파일이 벡터 DB에 성공적으로 저장되었습니다. 다른 도움이 필요하시면 알려주세요.\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `crawl_naver_and_embed` with `{'term': 'GPU'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3m✅ 'GPU'에 대한 뉴스 트렌드가 벡터 DB에 저장되었습니다.\u001b[0m\u001b[32;1m\u001b[1;3m'GPU'에 대한 뉴스 트렌드를 성공적으로 검색하고 벡터 DB에 저장했습니다. 다른 도움이 필요하시면 알려주세요.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'GPU 라는 키워드로 뉴스 트렌드를 검색하라.', 'output': \"'GPU'에 대한 뉴스 트렌드를 성공적으로 검색하고 벡터 DB에 저장했습니다. 다른 도움이 필요하시면 알려주세요.\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summarize_and_count` with `{'keyword': 'GPU'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m📘 PDF 요약: 1\n",
      "Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numero...\n",
      "🕸 웹 트렌드 요약: '네번째 퀀텀 점프' 울산서 시작…SK, AI 데이터센터 시대 연다\n",
      "이번 데이터센터는 서버랙당 2040kW의 전력을 소모하는 초고집적 GPU 기반 구조와 공랭·수랭 하이브리드 냉각시스템, 안정적인 네트워크를 갖춘 글로벌 최고 수준의 인프라로 설계됐다. 기존 데이터센터보다 410배...\n",
      "[위클리오늘] SKT-SK에코플랜트, 'SK AI 데이터센터 울산' 기공식 개최\n",
      "AI 데이터센터는 고성능 AI 연산을 위해 고전력, 냉각, 네트워크 역량을 갖춘 데이터센터로, 서버랙(Server Rack) 당 20~40kW 이상의 전력을 소비하는 고집...\n",
      "'📊 등장 횟수 - PDF: 7회, 웹: 6회\u001b[0m\u001b[32;1m\u001b[1;3m\"GPU\"라는 키워드는 PDF에서 7회, 웹에서 6회 등장했습니다.\n",
      "\n",
      "PDF 요약:\n",
      "재귀적 신경망, 특히 장기 기억 [13] 및 게이트 재귀 [7] 신경망은 언어 모델링 및 기계 번역 [35, 2, 5]과 같은 시퀀스 모델링 및 전사 문제에서 최첨단 접근법으로 확고하게 자리 잡았습니다.\n",
      "\n",
      "웹 트렌드 요약:\n",
      "'네번째 퀀텀 점프' 울산에서 시작…SK는 AI 데이터센터 시대를 연다. 이번 데이터센터는 서버 랙당 2040kW의 전력을 소모하는 초고집적 GPU 기반 구조와 공랭·수랭 하이브리드 냉각시스템, 안정적인 네트워크를 갖춘 글로벌 최고 수준의 인프라로 설계되었습니다. 기존 데이터센터보다 410배 더 높은 성능을 자랑합니다.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'GPU 라는 키워드에 대해 PDF와 웹을 요약하고 자주 나온 단어를 제시하라.', 'output': '\"GPU\"라는 키워드는 PDF에서 7회, 웹에서 6회 등장했습니다.\\n\\nPDF 요약:\\n재귀적 신경망, 특히 장기 기억 [13] 및 게이트 재귀 [7] 신경망은 언어 모델링 및 기계 번역 [35, 2, 5]과 같은 시퀀스 모델링 및 전사 문제에서 최첨단 접근법으로 확고하게 자리 잡았습니다.\\n\\n웹 트렌드 요약:\\n\\'네번째 퀀텀 점프\\' 울산에서 시작…SK는 AI 데이터센터 시대를 연다. 이번 데이터센터는 서버 랙당 2040kW의 전력을 소모하는 초고집적 GPU 기반 구조와 공랭·수랭 하이브리드 냉각시스템, 안정적인 네트워크를 갖춘 글로벌 최고 수준의 인프라로 설계되었습니다. 기존 데이터센터보다 410배 더 높은 성능을 자랑합니다.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#LangChain 최신 버전에서는 text splitter들이 langchain_text_splitter*라는 별도 모듈로 분리됨\n",
    "\n",
    "\n",
    "# 1️⃣ 함수들을 @tool로 정의 (또는 Tool(description=..., func=...) 형태로 래핑)\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"수식을 계산합니다. 예: '3+4*2'\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"계산 결과는 {result}입니다.\"\n",
    "    except Exception as e:\n",
    "        return f\"계산 오류: {e}\"\n",
    "\n",
    "# 다른 tool들도 동일하게 정의되어 있다고 가정 (embed_pdf_to_faiss, crawl_naver_and_embed, summarize_and_count)\n",
    "\n",
    "# 2️⃣ Tool 리스트 정의\n",
    "tools = [calculate, embed_pdf_to_faiss, crawl_naver_and_embed, summarize_and_count]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# 3️⃣ LLM 설정 (예: gpt-4)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 4️⃣ AgentExecutor 정의 : 실제로는 Agent가 단순히 Tool Calling만 하지는 않는다.\n",
    "# Agent 역할에 맞는 AI 서비스 처리를 한 결과를 가지고 다음 노드로 분기하거나 이어지며, 필요 시 Tool Calllin을 하게 된다.\n",
    "# LangChain을 이용한 Agent는, Tool Calling으로 세부 사항을 처리하거나 이전 노드의 출력(State)을 다음 노드에서 입력으로 받아 처리하도록 설계한다.\n",
    "executor = initialize_agent(\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5️⃣ 실행 (함수명 말고 자연어 입력으로 실행)\n",
    "print(executor.invoke({\"input\": \"다음 PDF 파일을 벡터 DB에 저장하라. 파일 경로는 './attention.pdf' 이다.\"}))\n",
    "print(executor.invoke({\"input\": \"GPU 라는 키워드로 뉴스 트렌드를 검색하라.\"}))\n",
    "print(executor.invoke({\"input\": \"GPU 라는 키워드에 대해 PDF와 웹을 요약하고 자주 나온 단어를 제시하라.\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
