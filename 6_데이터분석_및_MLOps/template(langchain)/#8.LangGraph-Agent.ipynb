{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93743ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# serpapi_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "\n",
    "# ë˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì§ì ‘ í‚¤ ì…ë ¥ (ê°œë°œ)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"\"  # ìì‹ ì˜ OpenAI í‚¤\n",
    "# os.environ[\"SERPAPI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeb0be",
   "metadata": {},
   "source": [
    "#### ë‹¤ìŒ ì‹¤ìŠµ ì½”ë“œëŠ” í•™ìŠµ ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš© ë°”ëë‹ˆë‹¤. ë¬¸ì˜ : audit@korea.ac.kr ì„ì„±ì—´ Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5d4b1",
   "metadata": {},
   "source": [
    "#### LangGraph ê¸°ë°˜ LangChain Agent êµ¬í˜„\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‹¤ìŒì˜ Toolë“¤ì„ ì‚¬ìš©í•˜ëŠ” LangGraph Agentë¥¼ êµ¬í˜„í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤:\n",
    "\n",
    "1. **ì‚¬ì¹™ì—°ì‚° í•¨ìˆ˜**\n",
    "2. **PDF ì—…ë¡œë“œ ë° ì„ë² ë”© í›„ FAISS ì €ì¥**\n",
    "3. **ì›¹ í¬ë¡¤ë§ì„ í†µí•œ íŠ¸ë Œë“œ ì €ì¥**\n",
    "4. **ë²¡í„° DBë¡œë¶€í„° ìš”ì•½ ë° ë“±ì¥ íšŸìˆ˜ ì¹´ìš´íŠ¸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš” ì‹œ ì£¼ì„ ì œê±°)\n",
    "%pip install langchain langgraph faiss-cpu openai PyMuPDF beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3746e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bbbd",
   "metadata": {},
   "source": [
    "#### 1. ì‚¬ì¹™ì—°ì‚° í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8e7975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"ì‚¬ì¹™ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ì˜ˆ: '2 + 3 * 4'\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"{expression} = {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"ì˜¤ë¥˜: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5542d2fe",
   "metadata": {},
   "source": [
    "#### 2. PDF ì„ë² ë”© ë° ë²¡í„° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15973137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ì „ì—­ ë²¡í„° DB\n",
    "pdf_vectorstore = None\n",
    "\n",
    "@tool\n",
    "def embed_pdf_to_faiss(path: str) -> str:\n",
    "    \"\"\"\n",
    "    PDF íŒŒì¼ì˜ ìƒëŒ€ ê²½ë¡œ ë˜ëŠ” ì ˆëŒ€ ê²½ë¡œë¥¼ ë°›ì•„ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•œ í›„,\n",
    "    í•´ë‹¹ PDFë¥¼ ì„ë² ë”©í•˜ì—¬ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    global pdf_vectorstore\n",
    "\n",
    "    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    abs_path = os.path.abspath(path)\n",
    "\n",
    "    if not os.path.exists(abs_path):\n",
    "        return f\"âŒ PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {abs_path}\"\n",
    "\n",
    "    loader = PyMuPDFLoader(abs_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    pdf_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    return f\"âœ… '{abs_path}' PDFê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd45d88",
   "metadata": {},
   "source": [
    "#### 3. ë„¤ì´ë²„ íŠ¸ë Œë“œ í¬ë¡¤ë§ ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99d74ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def crawl_trend_and_store(keyword: str) -> str:\n",
    "#     \"\"\"ë„¤ì´ë²„ ê²€ìƒ‰ íŠ¸ë Œë“œë¥¼ í¬ë¡¤ë§í•˜ê³  ë²¡í„° DBì— ì €ì¥í•œë‹¤.\"\"\"\n",
    "#     global web_vectorstore\n",
    "#     url = f\"https://search.naver.com/search.naver?query={keyword}\"\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "#     contents = [keyword + \":\\n\" + p for p in paragraphs if keyword.lower() in p.lower()]\n",
    "#     documents = [Document(page_content=c) for c in contents]\n",
    "#     embeddings = OpenAIEmbeddings()\n",
    "#     web_vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "#     return f\"âœ… '{keyword}' ê²€ìƒ‰ê²°ê³¼ê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "# 3ï¸âƒ£ ë„¤ì´ë²„ ê²€ìƒ‰ í¬ë¡¤ë§ + ì„ë² ë”©\n",
    "@tool\n",
    "def crawl_naver_and_embed(term: str) -> str:\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•´ íŠ¹ì • í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ê³ ,\n",
    "    ê·¸ ê²°ê³¼ë¥¼ ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    global web_vectorstore\n",
    "\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": \"gbqzUVViEiF6WXhuq3gZ\",\n",
    "        \"X-Naver-Client-Secret\": \"y0YXaa5unU\"\n",
    "    }\n",
    "\n",
    "    url = \"https://openapi.naver.com/v1/search/news.json\"\n",
    "    params = {\n",
    "        \"query\": term,\n",
    "        \"display\": 5,\n",
    "        \"sort\": \"date\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        return f\"âŒ ë„¤ì´ë²„ API í˜¸ì¶œ ì‹¤íŒ¨: {e}\"\n",
    "\n",
    "    data = response.json()\n",
    "    items = data.get(\"items\", [])\n",
    "\n",
    "    if not items:\n",
    "        return \"ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "\n",
    "    documents = []\n",
    "    for item in items:\n",
    "        title = re.sub(r\"<.*?>\", \"\", item.get(\"title\", \"\"))\n",
    "        description = re.sub(r\"<.*?>\", \"\", item.get(\"description\", \"\"))\n",
    "        content = f\"{title}\\n{description}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"source\": \"NaverNews\"}))\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "    docs = splitter.split_documents(documents)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    web_vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "    return f\"âœ… '{term}'ì— ëŒ€í•œ ë‰´ìŠ¤ íŠ¸ë Œë“œê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e3d9d0",
   "metadata": {},
   "source": [
    "#### 4. PDFì™€ ì›¹ ë²¡í„° DB ìš”ì•½ ë° ë¹ˆë„ìˆ˜ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb331cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def summarize_and_count(keyword: str) -> str:\n",
    "    \"\"\"PDFì™€ ì›¹ì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ ìš”ì•½í•˜ê³  ë“±ì¥ íšŸìˆ˜ë¥¼ ê³„ì‚°\"\"\"\n",
    "    pdf_docs = pdf_vectorstore.similarity_search(keyword, k=10)\n",
    "    web_docs = web_vectorstore.similarity_search(keyword, k=10)\n",
    "\n",
    "    pdf_text = \"\\n\".join([doc.page_content for doc in pdf_docs])\n",
    "    web_text = \"\\n\".join([doc.page_content for doc in web_docs])\n",
    "\n",
    "    def count_keyword(text):\n",
    "        return len(re.findall(keyword, text, re.IGNORECASE))\n",
    "\n",
    "    count_pdf = count_keyword(pdf_text)\n",
    "    count_web = count_keyword(web_text)\n",
    "\n",
    "    return f\"ğŸ“˜ PDF ìš”ì•½: {pdf_text[:300]}...\\nğŸ•¸ ì›¹ íŠ¸ë Œë“œ ìš”ì•½: {web_text[:300]}...\\n'ğŸ“Š ë“±ì¥ íšŸìˆ˜ - PDF: {count_pdf}íšŒ, ì›¹: {count_web}íšŒ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c2fbb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: /Users/phoenix/Eagle/2025_LangChain/langchain_lab\n",
      "attention.pdf ì¡´ì¬ ì—¬ë¶€: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬:\", os.getcwd())\n",
    "print(\"attention.pdf ì¡´ì¬ ì—¬ë¶€:\", os.path.exists(\"attention.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda72fc",
   "metadata": {},
   "source": [
    "#### 5. LangGraph Agent ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60639a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/0xbgl9wd3wz914nd58ylmykw0000gp/T/ipykernel_78154/2527526929.py:30: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  executor = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `embed_pdf_to_faiss` with `{'path': './attention.pdf'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mâœ… '/Users/phoenix/Eagle/2025_LangChain/langchain_lab/attention.pdf' PDFê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\u001b[0m\u001b[32;1m\u001b[1;3m'./attention.pdf' íŒŒì¼ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': \"ë‹¤ìŒ PDF íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ë¼. íŒŒì¼ ê²½ë¡œëŠ” './attention.pdf' ì´ë‹¤.\", 'output': \"'./attention.pdf' íŒŒì¼ì´ ë²¡í„° DBì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `crawl_naver_and_embed` with `{'term': 'GPU'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mâœ… 'GPU'ì— ëŒ€í•œ ë‰´ìŠ¤ íŠ¸ë Œë“œê°€ ë²¡í„° DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\u001b[0m\u001b[32;1m\u001b[1;3m'GPU'ì— ëŒ€í•œ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'GPU ë¼ëŠ” í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ê²€ìƒ‰í•˜ë¼.', 'output': \"'GPU'ì— ëŒ€í•œ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ì„±ê³µì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  ë²¡í„° DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `summarize_and_count` with `{'keyword': 'GPU'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mğŸ“˜ PDF ìš”ì•½: 1\n",
      "Introduction\n",
      "Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\n",
      "in particular, have been firmly established as state of the art approaches in sequence modeling and\n",
      "transduction problems such as language modeling and machine translation [35, 2, 5]. Numero...\n",
      "ğŸ•¸ ì›¹ íŠ¸ë Œë“œ ìš”ì•½: 'ë„¤ë²ˆì§¸ í€€í…€ ì í”„' ìš¸ì‚°ì„œ ì‹œì‘â€¦SK, AI ë°ì´í„°ì„¼í„° ì‹œëŒ€ ì—°ë‹¤\n",
      "ì´ë²ˆ ë°ì´í„°ì„¼í„°ëŠ” ì„œë²„ë™ë‹¹ 2040kWì˜ ì „ë ¥ì„ ì†Œëª¨í•˜ëŠ” ì´ˆê³ ì§‘ì  GPU ê¸°ë°˜ êµ¬ì¡°ì™€ ê³µë­Â·ìˆ˜ë­ í•˜ì´ë¸Œë¦¬ë“œ ëƒ‰ê°ì‹œìŠ¤í…œ, ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°–ì¶˜ ê¸€ë¡œë²Œ ìµœê³  ìˆ˜ì¤€ì˜ ì¸í”„ë¼ë¡œ ì„¤ê³„ëë‹¤. ê¸°ì¡´ ë°ì´í„°ì„¼í„°ë³´ë‹¤ 410ë°°...\n",
      "[ìœ„í´ë¦¬ì˜¤ëŠ˜] SKT-SKì—ì½”í”ŒëœíŠ¸, 'SK AI ë°ì´í„°ì„¼í„° ìš¸ì‚°' ê¸°ê³µì‹ ê°œìµœ\n",
      "AI ë°ì´í„°ì„¼í„°ëŠ” ê³ ì„±ëŠ¥ AI ì—°ì‚°ì„ ìœ„í•´ ê³ ì „ë ¥, ëƒ‰ê°, ë„¤íŠ¸ì›Œí¬ ì—­ëŸ‰ì„ ê°–ì¶˜ ë°ì´í„°ì„¼í„°ë¡œ, ì„œë²„ë™(Server Rack) ë‹¹ 20~40kW ì´ìƒì˜ ì „ë ¥ì„ ì†Œë¹„í•˜ëŠ” ê³ ì§‘...\n",
      "'ğŸ“Š ë“±ì¥ íšŸìˆ˜ - PDF: 7íšŒ, ì›¹: 6íšŒ\u001b[0m\u001b[32;1m\u001b[1;3m\"GPU\"ë¼ëŠ” í‚¤ì›Œë“œëŠ” PDFì—ì„œ 7íšŒ, ì›¹ì—ì„œ 6íšŒ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "PDF ìš”ì•½:\n",
      "ì¬ê·€ì  ì‹ ê²½ë§, íŠ¹íˆ ì¥ê¸° ê¸°ì–µ [13] ë° ê²Œì´íŠ¸ ì¬ê·€ [7] ì‹ ê²½ë§ì€ ì–¸ì–´ ëª¨ë¸ë§ ë° ê¸°ê³„ ë²ˆì—­ [35, 2, 5]ê³¼ ê°™ì€ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë° ì „ì‚¬ ë¬¸ì œì—ì„œ ìµœì²¨ë‹¨ ì ‘ê·¼ë²•ìœ¼ë¡œ í™•ê³ í•˜ê²Œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì›¹ íŠ¸ë Œë“œ ìš”ì•½:\n",
      "'ë„¤ë²ˆì§¸ í€€í…€ ì í”„' ìš¸ì‚°ì—ì„œ ì‹œì‘â€¦SKëŠ” AI ë°ì´í„°ì„¼í„° ì‹œëŒ€ë¥¼ ì—°ë‹¤. ì´ë²ˆ ë°ì´í„°ì„¼í„°ëŠ” ì„œë²„ ë™ë‹¹ 2040kWì˜ ì „ë ¥ì„ ì†Œëª¨í•˜ëŠ” ì´ˆê³ ì§‘ì  GPU ê¸°ë°˜ êµ¬ì¡°ì™€ ê³µë­Â·ìˆ˜ë­ í•˜ì´ë¸Œë¦¬ë“œ ëƒ‰ê°ì‹œìŠ¤í…œ, ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°–ì¶˜ ê¸€ë¡œë²Œ ìµœê³  ìˆ˜ì¤€ì˜ ì¸í”„ë¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ì„¼í„°ë³´ë‹¤ 410ë°° ë” ë†’ì€ ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'GPU ë¼ëŠ” í‚¤ì›Œë“œì— ëŒ€í•´ PDFì™€ ì›¹ì„ ìš”ì•½í•˜ê³  ìì£¼ ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ì œì‹œí•˜ë¼.', 'output': '\"GPU\"ë¼ëŠ” í‚¤ì›Œë“œëŠ” PDFì—ì„œ 7íšŒ, ì›¹ì—ì„œ 6íšŒ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.\\n\\nPDF ìš”ì•½:\\nì¬ê·€ì  ì‹ ê²½ë§, íŠ¹íˆ ì¥ê¸° ê¸°ì–µ [13] ë° ê²Œì´íŠ¸ ì¬ê·€ [7] ì‹ ê²½ë§ì€ ì–¸ì–´ ëª¨ë¸ë§ ë° ê¸°ê³„ ë²ˆì—­ [35, 2, 5]ê³¼ ê°™ì€ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ë° ì „ì‚¬ ë¬¸ì œì—ì„œ ìµœì²¨ë‹¨ ì ‘ê·¼ë²•ìœ¼ë¡œ í™•ê³ í•˜ê²Œ ìë¦¬ ì¡ì•˜ìŠµë‹ˆë‹¤.\\n\\nì›¹ íŠ¸ë Œë“œ ìš”ì•½:\\n\\'ë„¤ë²ˆì§¸ í€€í…€ ì í”„\\' ìš¸ì‚°ì—ì„œ ì‹œì‘â€¦SKëŠ” AI ë°ì´í„°ì„¼í„° ì‹œëŒ€ë¥¼ ì—°ë‹¤. ì´ë²ˆ ë°ì´í„°ì„¼í„°ëŠ” ì„œë²„ ë™ë‹¹ 2040kWì˜ ì „ë ¥ì„ ì†Œëª¨í•˜ëŠ” ì´ˆê³ ì§‘ì  GPU ê¸°ë°˜ êµ¬ì¡°ì™€ ê³µë­Â·ìˆ˜ë­ í•˜ì´ë¸Œë¦¬ë“œ ëƒ‰ê°ì‹œìŠ¤í…œ, ì•ˆì •ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ê°–ì¶˜ ê¸€ë¡œë²Œ ìµœê³  ìˆ˜ì¤€ì˜ ì¸í”„ë¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë°ì´í„°ì„¼í„°ë³´ë‹¤ 410ë°° ë” ë†’ì€ ì„±ëŠ¥ì„ ìë‘í•©ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, initialize_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#LangChain ìµœì‹  ë²„ì „ì—ì„œëŠ” text splitterë“¤ì´ langchain_text_splitter*ë¼ëŠ” ë³„ë„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ë¨\n",
    "\n",
    "\n",
    "# 1ï¸âƒ£ í•¨ìˆ˜ë“¤ì„ @toolë¡œ ì •ì˜ (ë˜ëŠ” Tool(description=..., func=...) í˜•íƒœë¡œ ë˜í•‘)\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"ìˆ˜ì‹ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì˜ˆ: '3+4*2'\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return f\"ê³„ì‚° ê²°ê³¼ëŠ” {result}ì…ë‹ˆë‹¤.\"\n",
    "    except Exception as e:\n",
    "        return f\"ê³„ì‚° ì˜¤ë¥˜: {e}\"\n",
    "\n",
    "# ë‹¤ë¥¸ toolë“¤ë„ ë™ì¼í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì • (embed_pdf_to_faiss, crawl_naver_and_embed, summarize_and_count)\n",
    "\n",
    "# 2ï¸âƒ£ Tool ë¦¬ìŠ¤íŠ¸ ì •ì˜\n",
    "tools = [calculate, embed_pdf_to_faiss, crawl_naver_and_embed, summarize_and_count]\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# 3ï¸âƒ£ LLM ì„¤ì • (ì˜ˆ: gpt-4)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# 4ï¸âƒ£ AgentExecutor ì •ì˜ : ì‹¤ì œë¡œëŠ” Agentê°€ ë‹¨ìˆœíˆ Tool Callingë§Œ í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤.\n",
    "# Agent ì—­í• ì— ë§ëŠ” AI ì„œë¹„ìŠ¤ ì²˜ë¦¬ë¥¼ í•œ ê²°ê³¼ë¥¼ ê°€ì§€ê³  ë‹¤ìŒ ë…¸ë“œë¡œ ë¶„ê¸°í•˜ê±°ë‚˜ ì´ì–´ì§€ë©°, í•„ìš” ì‹œ Tool Calllinì„ í•˜ê²Œ ëœë‹¤.\n",
    "# LangChainì„ ì´ìš©í•œ AgentëŠ”, Tool Callingìœ¼ë¡œ ì„¸ë¶€ ì‚¬í•­ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì´ì „ ë…¸ë“œì˜ ì¶œë ¥(State)ì„ ë‹¤ìŒ ë…¸ë“œì—ì„œ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„í•œë‹¤.\n",
    "executor = initialize_agent(\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5ï¸âƒ£ ì‹¤í–‰ (í•¨ìˆ˜ëª… ë§ê³  ìì—°ì–´ ì…ë ¥ìœ¼ë¡œ ì‹¤í–‰)\n",
    "print(executor.invoke({\"input\": \"ë‹¤ìŒ PDF íŒŒì¼ì„ ë²¡í„° DBì— ì €ì¥í•˜ë¼. íŒŒì¼ ê²½ë¡œëŠ” './attention.pdf' ì´ë‹¤.\"}))\n",
    "print(executor.invoke({\"input\": \"GPU ë¼ëŠ” í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ íŠ¸ë Œë“œë¥¼ ê²€ìƒ‰í•˜ë¼.\"}))\n",
    "print(executor.invoke({\"input\": \"GPU ë¼ëŠ” í‚¤ì›Œë“œì— ëŒ€í•´ PDFì™€ ì›¹ì„ ìš”ì•½í•˜ê³  ìì£¼ ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ì œì‹œí•˜ë¼.\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
