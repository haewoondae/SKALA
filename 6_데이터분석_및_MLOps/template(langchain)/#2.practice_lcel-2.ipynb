{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다음 실습 코드는 학습 목적으로만 사용 바랍니다. 문의 : audit@korea.ac.kr 임성열 Ph.D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 1. 동작에 필요한 패키지 설치 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv openai faiss-cpu langchain pypdf tiktoken\n",
    "%pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 2. LLM 연결 API Key 설정 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 3. PDF 파일에서 텍스트 추출 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Exp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def load_pdf_text(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# 사용 예\n",
    "pdf_path = \"./attention.pdf\"  # 같은 폴더에 PDF가 있어야 함\n",
    "raw_text = load_pdf_text(pdf_path)\n",
    "print(raw_text[:1000])  # 처음 1000자만 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4. 텍스트 분할 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89개 문단 생성됨\n",
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.create_documents([text])\n",
    "\n",
    "# 사용 예\n",
    "documents = split_text(raw_text)\n",
    "print(f\"{len(documents)}개 문단 생성됨\")\n",
    "print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 5. 벡터 임베딩 및 벡터 DB (FAISS) 인덱싱 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 임베딩 모델 생성\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# 벡터 DB 생성\n",
    "vector_db = FAISS.from_documents(documents, embedding_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 6. 사용자 쿼리에 대한 검색 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism rela...\n",
      "\n",
      "[2] Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "a...\n",
      "\n",
      "[3] The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "The\n",
      "Law\n",
      "will\n",
      "never\n",
      "be\n",
      "perfect\n",
      ",\n",
      "but\n",
      "its\n",
      "application\n",
      "should\n",
      "be\n",
      "just\n",
      "-\n",
      "this\n",
      "is\n",
      "what\n",
      "we\n",
      "are\n",
      "missing\n",
      ",\n",
      "in\n",
      "my\n",
      "opinion\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "Figure 5: Many of the attention heads exhibit ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_docs(query, db, k=3):\n",
    "    return db.similarity_search(query, k=k)\n",
    "\n",
    "# 예시 쿼리\n",
    "query = \"이 논문에서 제안한 Attention과 Self-Atteion은 무엇인가요?\"\n",
    "matched_docs = retrieve_docs(query, vector_db)\n",
    "for i, doc in enumerate(matched_docs):\n",
    "    print(f\"[{i+1}] {doc.page_content[:300]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 7. LLM으로 답변 생성 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 답변:\n",
      "\n",
      "Attention과 Self-Attention은 모두 시퀀스 모델링에서 중요한 역할을 하는 메커니즘이지만, 그 목적과 적용 방식에서 차이가 있습니다.\n",
      "\n",
      "1. **Attention**:\n",
      "   - Attention 메커니즘은 입력 또는 출력 시퀀스 내의 서로 다른 위치 간의 의존성을 모델링하는 데 사용됩니다. 이는 입력 시퀀스의 특정 부분에 더 많은 가중치를 부여하여 모델이 중요한 정보에 집중할 수 있도록 합니다.\n",
      "   - Attention은 주로 시퀀스-투-시퀀스 모델에서 사용되며, 특히 입력과 출력 시퀀스 간의 관계를 모델링하는 데 유용합니다.\n",
      "   - 전통적으로 Attention은 RNN과 같은 순환 신경망과 함께 사용되어 왔습니다. 이는 시퀀스의 길이에 관계없이 의존성을 모델링할 수 있도록 합니다.\n",
      "\n",
      "2. **Self-Attention**:\n",
      "   - Self-Attention, 또는 Intra-Attention은 단일 시퀀스 내의 서로 다른 위치 간의 관계를 모델링하는 메커니즘입니다. 이는 시퀀스의 각 위치가 다른 모든 위치와의 관계를 고려하여 시퀀스의 표현을 계산하는 데 사용됩니다.\n",
      "   - Self-Attention은 Transformer와 같은 모델에서 핵심적인 역할을 하며, 순환 구조 없이도 시퀀스 내의 모든 위치 간의 의존성을 효과적으로 모델링할 수 있습니다.\n",
      "   - Self-Attention은 특히 긴 시퀀스에서도 병렬 처리가 가능하다는 장점이 있어, 계산 효율성을 크게 향상시킵니다.\n",
      "\n",
      "논문에서는 Transformer 모델이 순환 구조를 배제하고 Self-Attention 메커니즘을 사용하여 시퀀스 내의 의존성을 모델링한다고 설명하고 있습니다. 이는 Attention 메커니즘이 RNN과 함께 사용되는 기존 방식과 대비됩니다. Self-Attention은 여러 헤드를 사용하여 다양한 관점에서 시퀀스를 분석할 수 있도록 하며, 이는 모델의 표현력을 높이는 데 기여합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def generate_answer(docs, user_query):\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"\"\"다음은 논문에서 발췌한 내용입니다:\\n\\n{context}\\n\\n사용자 질문: {user_query}\\n\\n논문 내용을 참고하여 질문에 답변해 주세요.\"\"\"\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.3)\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    return response.content\n",
    "\n",
    "# 사용자 입력 받기\n",
    "# query = input(\"질문을 입력하세요: \")\n",
    "query = \"Attention과 Self-Attention을 비교하시오.\"\n",
    "\n",
    "# 답변 생성\n",
    "answer = generate_answer(matched_docs, query)\n",
    "print(\"\\n💡 답변:\\n\")\n",
    "print(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
